{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":165355107,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport glob\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\npl.seed_everything(42) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextProcessor:\n    \"\"\"Handles converting text to numbers. Includes a fix to reset vocab.\"\"\"\n    def __init__(self, max_vocab=10000, max_len=50):\n        self.max_vocab = max_vocab\n        self.max_len = max_len\n        self.reset_vocab()\n        \n    def reset_vocab(self):\n        \"\"\"Clears the dictionary so we don't count words twice.\"\"\"\n        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n        self.vocab_size = 2\n        \n    def clean_text(self, text):\n        text = str(text).lower()\n        # Keep only letters and numbers\n        text = re.sub(r'[^a-z0-9\\s]', '', text)\n        return text\n\n    def build_vocab(self, text_list):\n        # FIX: Reset vocab every time build_vocab is called to prevent Index Errors\n        self.reset_vocab()\n        \n        all_words = []\n        for text in text_list:\n            clean = self.clean_text(text)\n            all_words.extend(clean.split())\n            \n        counts = Counter(all_words).most_common(self.max_vocab - 2)\n        \n        for word, _ in counts:\n            self.word2idx[word] = self.vocab_size\n            self.idx2word[self.vocab_size] = word\n            self.vocab_size += 1\n        print(f\"‚úÖ Vocab Built! Size: {self.vocab_size}\")\n\n    def text_to_sequence(self, text):\n        clean = self.clean_text(text)\n        # Convert words to IDs. Use 1 (<UNK>) if word not found.\n        seq = [self.word2idx.get(w, 1) for w in clean.split()]\n        \n        # Padding / Truncating\n        if len(seq) < self.max_len:\n            seq = seq + [0] * (self.max_len - len(seq))\n        else:\n            seq = seq[:self.max_len]\n        return torch.tensor(seq, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass GradingDataset(Dataset):\n    def __init__(self, stu, tea, lbl, proc):\n        self.data = list(zip(stu, tea, lbl))\n        self.proc = proc\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        s, t, l = self.data[idx]\n        return (self.proc.text_to_sequence(s), \n                self.proc.text_to_sequence(t), \n                torch.tensor(l, dtype=torch.float))\n\nclass GradingDataModule(pl.LightningDataModule):\n    def __init__(self, stu, tea, lbl, batch_size=32):\n        super().__init__()\n        self.batch_size = batch_size\n        self.full_data = list(zip(stu, tea, lbl))\n        random.shuffle(self.full_data)\n        self.processor = TextProcessor()\n        \n    def setup(self, stage=None):\n        # 1. Build Vocab on ALL data\n        stu_list, tea_list, _ = zip(*self.full_data)\n        self.processor.build_vocab(stu_list + tea_list)\n        \n        # 2. Split Data\n        total = len(self.full_data)\n        tr, va = int(total*0.8), int(total*0.9)\n        \n        # 3. Create Datasets\n        s, t, l = zip(*self.full_data)\n        self.train_ds = GradingDataset(s[:tr], t[:tr], l[:tr], self.processor)\n        self.val_ds = GradingDataset(s[tr:va], t[tr:va], l[tr:va], self.processor)\n        self.test_ds = GradingDataset(s[va:], t[va:], l[va:], self.processor)\n\n    def train_dataloader(self): return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n    def val_dataloader(self): return DataLoader(self.val_ds, batch_size=self.batch_size)\n    def test_dataloader(self): return DataLoader(self.test_ds, batch_size=self.batch_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    # Fixed argument list to match Tester\n    def __init__(self, emb, hid_dim, voc_size, n_rnn_layer, n_heads, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(voc_size, emb)\n        self.rnn = nn.LSTM(emb, hid_dim, num_layers=n_rnn_layer, batch_first=True, bidirectional=True, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        embedded = self.dropout(self.embedding(text))\n        _, (hidden, _) = self.rnn(embedded)\n        # Concatenate forward and backward final hidden states\n        return torch.cat((hidden[-2], hidden[-1]), dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Tester(nn.Module):\n    def __init__(self, emb_dim, hid_dim, voc_size):\n        super().__init__()\n        # Initialize Encoder correctly\n        self.encoder = Encoder(emb=emb_dim, hid_dim=hid_dim, voc_size=voc_size, n_rnn_layer=2, n_heads=4, dropout=0.3)\n        self.cos = nn.CosineSimilarity(dim=1)\n        self.classifier = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n\n    def forward(self, student_text, teacher_text): # Fixed typo 'forwerd' -> 'forward'\n        student_vec = self.encoder(student_text)\n        teacher_vec = self.encoder(teacher_text)\n        similarity = self.cos(student_vec, teacher_vec).unsqueeze(1)\n        return self.classifier(similarity)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass GradingLearner(pl.LightningModule):\n    def __init__(self, model_class, emb_dim, hid_dim, voc_size, lr=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = model_class(emb_dim, hid_dim, voc_size)\n        self.criterion = nn.MSELoss() # Changed to MSE for grading (0.0 to 1.0)\n        self.history = {\"train_loss\": [], \"val_loss\": []}\n\n    def forward(self, s, t): return self.model(s, t)\n\n    def training_step(self, batch, idx):\n        loss = self.criterion(self(batch[0], batch[1]).squeeze(), batch[2])\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, idx):\n        loss = self.criterion(self(batch[0], batch[1]).squeeze(), batch[2])\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return loss\n\n    def on_train_epoch_end(self):\n        self.history[\"train_loss\"].append(self.trainer.callback_metrics.get(\"train_loss\").item())\n        \n    def on_validation_epoch_end(self):\n        v_loss = self.trainer.callback_metrics.get(\"val_loss\")\n        if v_loss: self.history[\"val_loss\"].append(v_loss.item())\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n        # FIX: Removed 'verbose=True' causing TypeError\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\", \"interval\": \"epoch\", \"strict\": True}}\n\n    def plot_performance(self):\n        plt.figure(figsize=(10, 5))\n        plt.plot(self.history[\"train_loss\"], label=\"Train Loss\")\n        plt.plot(self.history[\"val_loss\"], label=\"Val Loss\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"\\n--- 1. Loading Data ---\")\ncsv_files = glob.glob(\"/kaggle/input/**/*.csv\", recursive=True)\nif not csv_files:\n    # Try Excel\n    excel_files = glob.glob(\"/kaggle/input/**/*.xlsx\", recursive=True)\n    if not excel_files:\n        raise FileNotFoundError(\"Please add a dataset in Kaggle UI (Sidebar -> Add Data)\")\n    df = pd.read_excel(excel_files[0])\nelse:\n    df = pd.read_csv(csv_files[0])\n\n# 2. DETECT COLUMNS\ndef find_col(k, c):\n    for col in c: \n        if any(x in col.lower() for x in k): return col\n    return None\n\ns_col = find_col(['student', 'answer'], df.columns) or 'Student Answer'\nt_col = find_col(['teacher', 'model', 'reference'], df.columns) or 'Model Answer'\nl_col = find_col(['score', 'grade'], df.columns) or 'Score'\n\nprint(f\"Columns Detected: Student='{s_col}', Teacher='{t_col}', Score='{l_col}'\")\n\ndf = df.dropna(subset=[s_col, t_col, l_col])\nraw_s = df[s_col].astype(str).tolist()\nraw_t = df[t_col].astype(str).tolist()\nraw_l = df[l_col].astype(float).tolist()\n\n\nif max(raw_l) > 1.0: raw_l = [x / max(raw_l) for x in raw_l]\n\n\ndm = GradingDataModule(raw_s, raw_t, raw_l, batch_size=32)\n\n\ndm.setup()\nactual_vocab_size = dm.processor.vocab_size\nprint(f\"Real Vocab Size: {actual_vocab_size}\")\n\n\nSAFE_VOCAB = actual_vocab_size + 100\nprint(f\"Initializing Model with Safe Vocab: {SAFE_VOCAB}\")\n\nmodel = GradingLearner(Tester, emb_dim=64, hid_dim=128, voc_size=SAFE_VOCAB)\n\n# 7. TRAIN\nprint(\"\\n--- 2. Starting Training ---\")\ntrainer = pl.Trainer(max_epochs=15, accelerator=\"auto\", devices=1)\ntrainer.fit(model, dm)\n\n# 8. PLOT\nprint(\"\\n--- 3. Results ---\")\nmodel.plot_performance()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef predict_grade(student_ans, teacher_ans):\n    \"\"\"\n    Takes a single student answer and teacher answer,\n    processes them, and returns the predicted score (0.0 to 1.0).\n    \"\"\"\n\n    model.eval()\n    model.to(\"cpu\")\n    \n\n    s_seq = dm.processor.text_to_sequence(student_ans).unsqueeze(0)\n    t_seq = dm.processor.text_to_sequence(teacher_ans).unsqueeze(0)\n    \n    with torch.no_grad(): \n        score = model(s_seq, t_seq)\n        \n    return score.item()\n\n\nprint(\"\\nü§ñ Grading AI Predictions:\\n\")\n\n# TEST CASE 1: The \"TRUE\" Case (Correct Answer)\n# Context: Biology/Cells\nteacher_ref = \"The mitochondria is the powerhouse of the cell.\"\nstudent_good = \"Mitochondria are responsible for producing energy for the cell.\"\n\nscore_1 = predict_grade(student_good, teacher_ref)\n\nprint(f\"üìù Teacher: '{teacher_ref}'\")\nprint(f\"‚úÖ Student (Good): '{student_good}'\")\nprint(f\"üìä Predicted Score: {score_1:.4f}  (Expected: High)\\n\")\nprint(\"-\" * 50 + \"\\n\")\n\n\n# TEST CASE 2: The \"FALSE\" Case (Wrong Answer)\n# Context: Biology/Cells\nstudent_bad = \"The mitochondria is the brain of the cell and controls DNA.\"\n\nscore_2 = predict_grade(student_bad, teacher_ref)\n\nprint(f\"üìù Teacher: '{teacher_ref}'\")\nprint(f\"‚ùå Student (Bad):  '{student_bad}'\")\nprint(f\"üìä Predicted Score: {score_2:.4f}  (Expected: Low)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\n\nprint(\"üíæ Saving System...\")\n\n# 1. Save the Dictionary (Vocabulary)\n# We need this to translate new student answers into numbers later\nvocab_path = \"vocab.json\"\nwith open(vocab_path, \"w\") as f:\n    json.dump(dm.processor.word2idx, f)\nprint(f\"‚úÖ Saved Vocabulary to: {vocab_path}\")\n\n# 2. Save the Brain (Model Weights)\n# We save the internal 'Tester' model, not the whole Lightning wrapper\n# This makes it easier to load in a simple Python app later\nmodel_path = \"grading_model.pt\"\ntorch.save(model.model.state_dict(), model_path)\nprint(f\"‚úÖ Saved Model Weights to: {model_path}\")\n\nprint(\"\\nüéâ DONE! You can now download these files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T14:05:16.640545Z","iopub.execute_input":"2026-01-27T14:05:16.640932Z","iopub.status.idle":"2026-01-27T14:05:16.651773Z","shell.execute_reply.started":"2026-01-27T14:05:16.640897Z","shell.execute_reply":"2026-01-27T14:05:16.651026Z"}},"outputs":[{"name":"stdout","text":"üíæ Saving System...\n‚úÖ Saved Vocabulary to: vocab.json\n‚úÖ Saved Model Weights to: grading_model.pt\n\nüéâ DONE! You can now download these files.\n","output_type":"stream"}],"execution_count":5}]}